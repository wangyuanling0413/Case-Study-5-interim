---
title: "Case Study 5 Interim Report"
author: "Yuanling Wang, Irina Cristali"
date: "11/14/2018"
output: pdf_document
---

## Introduction

This report aims to explore the missing data patterns and mechanisms of a dataset of cardiac catheterization procedures provided by the Duke Databank for Cardiovascular Disease (DDCD), a clinical care database created by Duke Department of Medicine from 1946 to 1967. The patterns of missing data would be closely examined through visualizations and statistical tests; approaches of handling the missing data including imputation and necessary deletion would follow. The potential for missing data to bias analysis results would also be discussed. 

```{r, echo = FALSE, include = FALSE}
library(mice)
library(VIM)
library(lattice)
```

## Data Preprocessing

The dataset for analysis is $DUKECATHR$, the version of $DUKECATH$ dataset for educational use. The dataset contains a subset of cardiac catheterization procedures conducted in adult patients at Duke University Medical Center between 1985 and 2013. The observed variables for each patient include catheterization identification and linking, demographics information, patient history, laboratory results prior to the catheterization, catheterization procedure, catheterization results and the follow-up information. 

We start by reading in the data file which contains 83320 observations. As described in the "DUKECATHR" dataset documentation for users, each observation in the dataset corresponds to a certain catheterization procedure undergone by a patient at the Duke Hospital. However, as we have noted in class and read in the documentation, there are several different procedures that correspond to the same patient. Since the main goal of our investigation will be to determine which factors impact survival after the first catheterization, we will thus subset the data and only account for the first procedure, in the case of patients that have undergone multiple ones. 

```{r, echo = FALSE, include = FALSE}
dukecathr <- read.csv(file="dukecathr.csv", header=TRUE, sep=",")
nrow(dukecathr)
cath1a=subset(dukecathr, dukecathr$RSEQCATHNUM==1)
#View(cath1a)
```

```{r, echo = FALSE, include = FALSE}
nrow(cath1a)
```


<<<<<<< HEAD
After this operation, we notice that the dimension of our data has decreased considerably to only 39098 rows. 
=======
After this operation, we can now notice that the dimension of our data has decreased considerably to only 39098 rows, which should help accelerate the running time of the data imputation algorithm we are going to use. 
>>>>>>> 8f2da264ee1500f627ef8889b5cb56a0784dd91f

We next investigate which variables in our dataset contain missing values, by firstly running a "summary" of our data, which currently contains a total of 52 columns. This command will indicate to us basic statistics for each variable, as well as whether NA's are present, which is a useful way to start exploring the "missingness" in our data. As we can see from the results, the variables containing missing values are: RACE_G (Race), CHFSEV (CHF severity), DPCABG (Days to Closest Previous Coronary Artery Bypass Surgery), DPMI (Days to Closest Previous Myocardial Infarction), DPPCI (Days to Closest Previous Percutaneous Coronary Intervention), HXANGINA (History of Angina), HXCHF (History of CHF), DIASBP_R (Diastolic Blood Pressure), PULSE_R (Heart Rate), SYSBP_R (Systolic Blood Pressure), CBRUITS (Carotid Bruits), HEIGHT_R (Height in cm), S3 (Third Heart Sound), WEIGHT_R (Weight in kg), CREATININE_R (Serum Creatinine), HDL_R (High Density Lipid), LDL_R (Low Density Lipid), TOTCHOL_R (Total Cholesterol), CORDOM (Coronary Dominance), GRAFTST (Maximum Stenosis in any Graft), LADST (Maximum Stenosis of the Left Anterior Descending Artery), LCXST (Maximum Stenosis of the Left Circumflex Artery), LMST (Maximum Stenosis of the Left Main Artery), LVEF_R (Left Ventricular Ejection Fraction %), PRXLADST (Maximum Stenosis of the Proximal Left Anterior Descending Artery), RCAST (Maximum Stenosis of the Right Coronary Artery), DSCABG (Days to First Subsequent Coronary Artery Bypass Surgery), DSMI (Days to First Subsequent Non-Fatal Myocardial Infarction), DSPCI (Days to First Subsequent Percutaneous Coronary Intervention), DSSTROKE (Days to First Subsequent Non-Fatal Stroke), that is, a total of 30 variables, more than half of all the variables in the dataset. 

```{r, echo = FALSE, include = FALSE}
summary(cath1a)
```

The natural question that follows is: for each of the above variables, what is the percentage of missing values? We investigate that and obtain the following missingness percentages for each variable that contains missing instances: RACE_G (1.9105837%), CHFSEV (1.7213157%), DPCABG (88.4162873%), DPMI (50.2353062%), DPPCI (96.6878101%), HXANGINA (0.2046140%), HXCHF (1.5269323%), DIASBP_R (3.6881682%), PULSE_R (0.8184562%), SYSBP_R (3.2866131%), CBRUITS (0.6649957%), HEIGHT_R (1.8108343%), S3 (0.4066704%), WEIGHT_R (1.7187580%), CREATININE_R (29.2367896%), HDL_R (69.0725868%), LDL_R (71.8272034%), TOTCHOL_R (68.3103995%), CORDOM (0.1048647%), GRAFTST (91.2246151%), LADST (8.4684639%), LCXST (10.652718%), LMST (5.9491534%), LVEF_R (29.4797688%), PRXLADST (8.4096373%), RCAST (9.6987058%), DSCABG (63.3178168%), DSMI (87.3139291%), DSPCI (49.2864085%), DSSTROKE (89.1273211%).

```{r, echo = FALSE, include = FALSE}
#code for finding the percentage of missing values
pMiss <- function(x){sum(is.na(x))/length(x)*100}
apply(cath1a,2,pMiss)
#apply(cath1a,1,pMiss)
```

As we can see from the above results, the majority of the variables having missing values (17 out of 30) actually have a reasonably small percentage of missingness, that is, less than or equal to 10%. However, the rest of the variables (with the exception of 2 variables having 20% and 29% missing values), have incredibly high percentages of missingness, which are greater than 50%. This determines us to consider dropping these latter variables, as the highly confidential nature of our dataset would likely prevent us from obtaining further measurements. Indeed, as written in the description of the DUKECATHR dataset, the data were grouped by race, age, and year in which the cardiac catheterization procedure was performed, in order to remove individually identifiable information, which prevents obtaining more information for our purposes. Before we firmly decide whether to drop these columns with exceedingly high numbers of missing values, we will visually explore the patterns of missing data more in depth in the following section. 


## Visualizations of missing data

We now proceed to examine the missing data pattern through data visualizations. To begin with, we group the variables by group considering the way they were collected; within each group, we look at the proportions and distributions of the missing data as well as the missing data pattern for different variables combined. The first three columns of the data on the catheterization identification are complete and were thus excluded from the visualization. 


```{r, include=FALSE, echo = FALSE}
library(mice)
library(VIM)
library(lattice)
```


We first use the "md.pattern" function which gives us an overview of the missing data patterns in the form of a dataframe, that we can visualize separately. The first row will tell us how many samples have complete information for all variables, whereas the following rows will tell us how many observations there are for each combinations of missing variables. 

We first try to get an overview of the missing data patterns in the form of the whole dataframe through $md.pattern$. The first row will tell us how many samples have complete information for all variables, whereas the following rows will tell us how many observations there are for each combinations of missing variables. 


```{r, echo = FALSE, include = FALSE}
#View(md.pattern(cath1a_2))
```

However, due to the high dimensions of the dataframe resulting from the above step (186 rows and 39 columns), the resulting plot could not convey useful information; we thus decided to group the variables and assess missingness in the dataset graphically, using the "aggr()" function.

Specifically, we looked into the interpretation of each variable and considered the possibility of the correlation of missing data patterns among them. The data collected can be grouped by the catheterization identification and linking, demographics information, patient history, laboratory results prior to the catheterization, catheterization procedure, catheterization results and the follow-up information, assuming that the correlation of missing data pattern would be more correlated within each group (which makes intuitive sense because within each group the data are collected through similar means, and are thus more likely to have similar missing patterns). We then visualize the missing data by group, with variables within each group examined in a combined fashion. An example of the visualization of vital sign variables is presented below.

```{r, echo = FALSE, include = FALSE}
#cath time and demographics
cath1a_agg1 = aggr(cath1a[,4:7],col=mdc(1:2),numbers=TRUE,sortVars=TRUE,
labels=names(cath1a[,4:7]),cex.axis=.7, gap=3, 
ylab=c("Proportion missing","Missingness Pattern"))

#patient history 1
cath1a_agg2 = aggr(cath1a[,8:12],col=mdc(1:2),numbers=TRUE,sortVars=TRUE,
labels=names(cath1a[,8:12]),cex.axis=.7, gap=3, 
ylab=c("Proportion missing","Missingness Pattern"))

#patient history 2
cath1a_agg3 = aggr(cath1a[,13:22],col=mdc(1:2),numbers=TRUE,sortVars=TRUE,
labels=names(cath1a[,13:22]),cex.axis=.7, gap=3, 
ylab=c("Proportion missing","Missingness Pattern"))

#vital signs
cath1a_agg4 = aggr(cath1a[,23:29],col=mdc(1:2),numbers=TRUE,sortVars=TRUE,
labels=names(cath1a[,23:29]),cex.axis=.7, gap=3, 
ylab=c("Proportion missing","Missingness Pattern"))

#lab results prior to cath
cath1a_agg5 = aggr(cath1a[,30:33],col=mdc(1:2),numbers=TRUE,sortVars=TRUE,
labels=names(cath1a[,30:33]),cex.axis=.7, gap=3, 
ylab=c("Proportion missing","Missingness Pattern"))

#cath procedure
cath1a_agg6 = aggr(cath1a[,34:36],col=mdc(1:2),numbers=TRUE,sortVars=TRUE,
labels=names(cath1a[,34:36]),cex.axis=.7, gap=3, 
ylab=c("Proportion missing","Missingness Pattern"))

#cath results
cath1a_agg7 = aggr(cath1a[,37:45],col=mdc(1:2),numbers=TRUE,sortVars=TRUE,
labels=names(cath1a[,37:45]),cex.axis=.7, gap=3, 
ylab=c("Proportion missing","Missingness Pattern"))

#follow-up
cath1a_agg8 = aggr(cath1a[,46:52],col=mdc(1:2),numbers=TRUE,sortVars=TRUE,
labels=names(cath1a[,46:52]),cex.axis=.7, gap=3, 
ylab=c("Proportion missing","Missingness Pattern"))
```

```{r}
#vital signs (demonstrated example)
cath1a_agg4 = aggr(cath1a[,23:29],col=mdc(1:2),numbers=TRUE,sortVars=TRUE,
labels=names(cath1a[,23:29]),cex.axis=.7, gap=3, 
ylab=c("Proportion missing","Missingness Pattern"))
```
 
From the visualization above, we notice that there are several pairs of variables whose missing data patterns suffer from the tendency of being correlated with each other. We decided to further look into each pair of them through margin plot so as to asses whether our data are missing completely at random, an essential assumption for deciding whether we can use the MICE method for imputation. However, we were able to find a counterexample for the variables $SYSBP_R$ and $WEIGHT_R$, were the red boxplots and blue boxplots are not exactly that similar. 

```{r}
marginplot(cath1a[, c("SYSBP_R", "WEIGHT_R")], col = mdc(1:2), cex.numbers = 1.2, pch = 19)

```
This may suggest that the data may not be missing completely at random, yet more evidence is needed to support the assumption, which we will pursue in a later section using Little's Test. 


## Data cleaning

Having performed the analysis above, we now return back to our question introduced in the "Data Preprocessing" section, namely whether we should remove the variables that have a high percent (>20%) of missing values from our dataset. Indeed, due to the nature of our final research question---aiming to determine which factors are likely to cause death or survival following a catheterization procedure--- each physical and medical condition information related to an individual play an important role, so we shoudl strive to save as much data as possible. 

However, we decide (with reservations) that the incredibly high percentages of missing values for some of the variables in the dataset, together with the plethora of combinations of missing variables for a single observation that we have investigated in the previous section, both represent a tremendous challenge in making reliable statistical investigations based on this dataset. In addition, the visual methods we have employed provided very little detail whether our missing data is MCAR (the best case scenario), MAR, or MNAR, preventing us to determine which course of action would be most appropriate with regards to the variables having a high missingness percentage. Due to these considerations, we decided to delete the columns where more than 20% of the data were missing, leaving us with columns having the more than reasonable level of 10% missing values. 

```{r, echo = FALSE, include = FALSE}

## code for deletion given the given threshold set

num_NA = rep(0,ncol(cath1a))

for (i in 1:length(num_NA)){
  num_NA[i] = sum(is.na(cath1a[,i]))
}

prop_NA = num_NA/nrow(cath1a)
propNA_under10perc = prop_NA <= 0.1
index_propNAover = which(propNA_under10perc==FALSE)

cath1a_2 = cath1a[,-index_propNAover]
#research on people's common threshold
```


Having considerably reduced our variables containing missing values, the next step is to finish cleaning up the data, by converting several variables taking numerical values to categorical ones, in order to match their discription in the DUKECATHR Dataset Dictionary. Moreover, having the right data type will also be essential in order to correctly implement a missing data imputation method, as we will see in the following section. As such, we convert numerous variables, including age, smoking status, race, catheterization procedure year, history of diabetes etc. to factors. The large number of categorical variables may also be due to the fact that some patients' credentials were grouped into broader categories in order to preserve anonimity.  

```{r, echo = FALSE, include = FALSE}
cath1a_2$YRCATH_G = factor(cath1a_2$YRCATH_G)
cath1a_2$AGE_G = factor(cath1a_2$AGE_G)
cath1a_2$RACE_G = factor(cath1a_2$RACE_G)
cath1a_2$ACS = factor(cath1a_2$ACS)
cath1a_2$CHFSEV = factor(cath1a_2$CHFSEV)
cath1a_2$HXANGINA = factor(cath1a_2$HXANGINA)
cath1a_2$HXCEREB = factor(cath1a_2$HXCEREB)
cath1a_2$HXCHF = factor(cath1a_2$HXCHF)
cath1a_2$HXCOPD = factor(cath1a_2$HXCOPD)
cath1a_2$HXDIAB = factor(cath1a_2$HXDIAB)
cath1a_2$HXHTN = factor(cath1a_2$HXHTN)
cath1a_2$HXHYL = factor(cath1a_2$HXHYL)
cath1a_2$HXSMOKE = factor(cath1a_2$HXSMOKE)
cath1a_2$S3 = factor(cath1a_2$S3)
cath1a_2$CATHAPPR = factor(cath1a_2$CATHAPPR)
cath1a_2$DIAGCATH = factor(cath1a_2$DIAGCATH)
cath1a_2$INTVCATH = factor(cath1a_2$INTVCATH)
cath1a_2$CORDOM = factor(cath1a_2$CORDOM)
cath1a_2$NUMDZV = factor(cath1a_2$NUMDZV)
cath1a_2$DEATH = factor(cath1a_2$DEATH)
cath1a_2$FUPROTCL = factor(cath1a_2$FUPROTCL)
```


## Little's test:

Having cleaned up the data, and reduced it to variables having reasonable percentages of missing values, the next step is to (at least) test whether the data is Missing Completely at Random (MCAR), as our previous investigations failed to determine whether the data are MAR or MNAR. In order to accomplish this, we will use Little's MCAR test, based on the "BaylorEdPsych" library, whose null hypothesis is that the data are missing completely at random. The p-value we obtain from this test is 0 (it is likely that the actual value was very close to zero and rounded off automatically), less than 0.05, we can conclude that our data are not missing completely at random. 

```{r, echo = FALSE, include = FALSE}
library(BaylorEdPsych)
test = LittleMCAR(cath1a_2)
test$df
test$chi.square
test$p.value
test$missing.patterns
test$amount.missing
```


## Handling missing data through imputation

Although the result if the previous section, namely that we failed to establish that the data was missing completely at random does not immediately indicate to us that we should use the MICE (Multiple Imputation using Chained Equations) method. Nonetheless, as we discussed in the data cleaning part, our current dataset only includes variables with less than 10 percent of data missing, so it would be reasonable to perform multiple imputation so that we do not lose valuable data information due to the presence of an acceptable portion of missing values. 

In order to reduce the computation time, we set the number of chained iterations in the "mice()" function to be equal to 5 (a more reasonable value would have been 10, since that would match our percentage of missing values). 


```{r, echo = FALSE, include = FALSE}
imp <- mice(cath1a_2, m=5, printFlag=FALSE, maxit = 5, seed=2525) 
```

Furthermore, since we have changed the numerous variables to their (correct) categorical type in the data cleaning section, when checking the 

```{r, echo = FALSE, include = FALSE}
imp$method
```


```{r, echo = FALSE}
densityplot(imps)
```

```{r, echo = FALSE}
xyplot(imp, PULSE_R  ~ RACE_G+CHFSEV+HXANGINA+HXCHF+DIASBP_R+SYSBP_R+CBRUITS+HEIGHT_R+S3+WEIGHT_R+CORDOM+LADST+LMST+PRXLADST+RCAST,pch=18,cex=1)
```

```{r, echo = FALSE}
xyplot(imp, HEIGHT_R  ~ RACE_G+CHFSEV+HXANGINA+HXCHF+DIASBP_R+SYSBP_R+CBRUITS+PULSE_R+S3+WEIGHT_R+CORDOM+LADST+LMST+PRXLADST+RCAST,pch=18,cex=1)
```

## Potential Bias

There are several ways in which the missing data can potentially bias the analysis results. Firstly, as we deleted the columns with more than 10 percent of data missing, we are losing valuable information, particularly patient's history and laboratory results before the tests. These missed variables can be of great importance when we perform survival analysis later on, because they are all related to the patient's health condition. What's more, when we performed multiple imputation of our remaining dataset, we assumed that our data is missing at random (although not completely) -- which was justified by the fact that we are only dealing with data missing less than 10 percent. However, such approximation has
